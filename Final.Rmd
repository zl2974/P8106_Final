---
title: "????????????"
author: "Iris Ziyi Wang, Yiming Zhao, Jeffrey Zhuohui Liang"
date: "4/7/2021"
output: pdf_document
---

# Introduction

  Cardiovascular disease is the leading disease burden in U.S, according to \textit{www.cdc.com} \cite{cdccardio}, on average one person die from heart disease every 36 seconds. And 1 in 4 death is caused by cardiovascular disease. Heavy disease burden of cardiovascular disease should be manage to improve population health.
    
  One of many important manners is screening, \textit{American Heart Association}\cite{AHA} lists important screening to help monitor heart condition, which are shown below:
\begin{itemize}
\item Blood Pressure
\item Fasting Lipoprotein Profile
\item Body Weight
\item Blood Glucose
\item Smoking, physical activity, diet
\end{itemize}

 
  In light of aiding the screening process, we will use `Heart Disease Data Set` from UCI \cite{heartdiseasedata} to build our models and select one for applications.


  The `Heart Disease Data` is a dataset with 76 attributes, all data were collected from 4 sites, namely Cleveland, Hungary, Switzerland, and the VA Long Beach. Of all 76 attributes, we selected 14 variables as our training data in this case as there're previously researchs have done similar job and used these 14 pre-selected variables. 
  The predictors used are:

* age: The person’s age in years
* sex: The person’s sex (1 = male, 0 = female)
* cp: chest pain type
  — Value 0: asymptomatic
  — Value 1: atypical angina
  — Value 2: non-anginal pain
  — Value 3: typical angina
* trestbps: The person’s resting blood pressure (mm Hg on admission to the hospital)
*  chol: The person’s cholesterol measurement in mg/dl
*  fbs: The person’s fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)
*  restecg: resting electrocardiographic results
  — Value 0: showing probable or definite left ventricular hypertrophy by Estes’ criteria
  — Value 1: normal
  — Value 2: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)
* thalach: The person’s maximum heart rate achieved
* exang: Exercise induced angina (1 = yes; 0 = no)
* oldpeak: ST depression induced by exercise relative to rest (‘ST’ relates to positions on the ECG plot. See more here)
* slope: the slope of the peak exercise ST segment 
  — 0: downsloping; 
  - 1: flat; 
  - 2: upsloping
* ca: The number of major vessels (0–3)
* thal: A blood disorder called thalassemia Value 0: NULL (dropped from the dataset previously
  - Value 1: fixed defect (no blood flow in some part of the heart)
  - Value 2: normal blood flow
  - Value 3: reversible defect (a blood flow is observed but it is not normal)
* target: Heart disease (1 = no, 0= yes)


# Exploratory Analysis

  From above plot, some features are well distinguish for disease status, eg. `num_major_vessels_flouro `, `chest_pain_typeatypical angina`,`st_depression_exercise`, these variables may be statistical significant for the model.
  

# Model

  As shown, there are missing values in our data. Assuming that these values are missing at random, we impute these values with `knnImpute` method. All data were center and scale before training.

  To train classifiers, we choose `Elastic Net  logistics`, `MARS`, `KNN`, `LDA`, `QDA` and `TREE` models to train our data with 5-fold cross validation.
  
  When training, `ROC` is used as loss function for our model, as we intent to build a model with highest classification ability to predict whether a client has heart disease. 
  
## Model tunning

  Since variables in the data set were selected as important influence factors of heart disease, all of 13 variables were included in the models as predictors. Because the KNN method s applicable in any of Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR) assumptions, the missing value would be estimated by knn method. 

  In order to select best model to predictor heart disease status, this report compare the ROC value of logistic regression, MARS,KNN, LDA, QDA, SVM and tree models. These models are flexible enough to capture the underlying truth by adjusting appropriate tuning parameters.The train function was used to select the optimal tuning parameters by 5-fold cross validation for elastic net logistics regression, Mars, KNN, SVM and tree models by choosing the tuning parameter with highest ROC in ROC plot.  

  For KNN, although the missing values are imputed before applying KNN model to train data, major proportion of lossing the value of peak Exercise ST Segment,Number of visible vessels may still degrade the model performance. Moreover, the heart disease training data has 691 observations. Because KNN is a distance-based algorithm, there is a high cost of calculating distance between a new point and each existing point, which might degrade the performance of the algorithm [5].

  Elastic net logistics regression is logistics regression which loss function is modified with L1 penalty and L2 penalty, we tune this penalty term $\lambda$ and the elastic net parameter $\alpha$ for regression model training with cross-validation.
  
  MARS has model predictors' order and prune remaining term as parameters for tuning. Assuming that data can be well-explain with at most cubic model, we tune the order from 1-3 and leaving cross validation to choose for prune term.
  
  Tree model and the ensemble form of tree's family were consider in the model. For single tree model, we tune the complexity of the tree, the model was selected with "1-se" rule to have the simplest working model. Random Forest(and Bagging) ensemble trees with randomly sampling variable in each split into a final tree. Model tunes with number of predictors sampled at each split, splitting rules and minimum node size. Boosting use sequential addictive tree models and ensemble into a tree. Model tuning with number of trees and learning rates.
  
  Support Vectors Machines fit boundary on the support vectors, both linear kernal and radial kernal were consider in the model. 
  
  Models is listed in Table.3.
  
  

# Result

  In the MARS model, `chest pain type: atypical angina` has the highest importance, followed by `serum cholesterol` and `st depression excercise`. `fasting blood sugar` has second lowest importance to AUC loss in the MARS model followed by `resting ECG: ST-T wave abnormality`.
  
  The models, included others not selected model is test against the test data. The test performance is similar to the train performance. Which `knn` has the highest ROC, but is similar or not significantly different from other methods except for tree, which has the poorest performance.


# Conclusion

  MARS model has high predictability and high sensitivity, which is suitable for screening. The MARS model, with it's nature of spline predictor, also provide good reference for critical values for labs/ testing result for diagnosis. 
  
  The first implement of the model is to predict/diagnosis heart disease for the screening visitors. 
  
  The Second implement of the model is to selected high importance predictors as screening items. For example, `chest pain type` has the most 2 important preditors in the model, as well as `serum cholesterol` and `excercise`, which should included in the screening process. Although the important score plot shows that vessels numbers are very important variables in predicting heart disease, the validity of this result is questionable because of the large missing values of vessel number in the data. Moreover, `fast boold sugar` and `resting ECG: ST-T wave abnormality` has minor importance to the model, which can consider removing for economic package. For example, in the listed recommended screening terms of \cite{AHA}, `Blood Glucose` can be consider remove from screening, as it has little use for our model, but it may has clinical usage which is not our consideration in the projects.
  
  From the important score plot, age(28-77)  is not a important variable in predicting the response. This indicates that heart disease may not have age preference. This result is counterintuitive since people usually think that the risk of heart disease would be increased with age. Our analysis result implies that young people also have risk of heart disease as same as old people. It is corresponding with a multi-state study that investigated 28,000 people hospitalized for heart attacks from 1995 to 2014[4]. The research claims that 30 percent of those patients were young, age 35 to 54. Therefore, it is also important for young people to take care of their heart problems.  
  
   One of potential limitation for the analysis result is that the raw data was created by combining 4 different location heart disease data. Further more investigation may be necessary to make sure the generalizability of heart disease result in different locations.
  
# Reference

\begin{thebibliography}{9}
\bibitem{cdccardio}
Centers for Disease Control and Prevention:
\url{https://www.cdc.gov/heartdisease/facts.htm#:~:text=Heart%20Disease%20in%20the%20United%20States&text=One%20person%20dies%20every%2036,1%20in%20every%204%20deaths.&text=Heart%20disease%20costs%20the%20United,year%20from%202014%20to%202015.}


\bibitem{AHA}
American Heart Association:Heart-Health Screenings:

\bibitem{heartdiseasedata}
USI:
http://archive.ics.uci.edu/ml/datasets/heart+Disease
\end{thebibliography}

[4]
[ Twenty Year Trends and Sex Differences in Young Adults Hospitalized With Acute Myocardial Infarction](https://www.ahajournals.org/doi/full/10.1161/CIRCULATIONAHA.118.037137)


\newpage
# Appendix

```{r setup, include=FALSE}
set.seed(123123)
library(reticulate)
library(caret)
library(glmnet)
library(pls)
library(splines)
library(mgcv)
library(pdp)
library(earth)
library(doParallel)
library(patchwork)
library(DALEX)
library(tidyverse)
knitr::opts_chunk$set(
  fig.height = 8,
  fig.width = 10,
  message = F,
  echo = F,
  warning = F,
  cache = T
)
theme_set(theme_minimal() + theme(
  legend.position = "bottom",
  plot.title = element_text(hjust = 0.5)
))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  digits = 3
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```





```{r load_data,echo=F}
hrt_data =
  tibble(
    location = c("cle", "swi", "va", "hun"),
    file =
      c(
        "./data/processed.cleveland.data",
        "./data/processed.switzerland.data",
        "./data/processed.va.data",
        "./data/processed.hungarian.data"
      )
  ) %>%
  mutate(location = as.factor(location),
         data = map(file,
                    ~ read_csv(here::here(.x), col_names = F,
                               col_types = "dddddddddddddd"))) %>% 
  unnest(data)

# change column's name
colnames(hrt_data)[3:16] = c(
  "Age",
  "Sex",
  "Chest_Pain_Type",
  "Resting_Blood_Pressure",
  "Serum_Cholesterol",
  "Fasting_Blood_Sugar",
  "Resting_ECG",
  "Max_Heart_Rate_Achieved",
  "Exercise_Induced_Angina",
  "ST_Depression_Exercise",
  "Peak_Exercise_ST_Segment",
  "Num_Major_Vessels_Flouro",
  "Thalassemia",
  "Diagnosis_Heart_Disease"
)

hrt_data  = hrt_data %>%
  janitor::clean_names() %>%
  select(-file) %>%
  mutate(
    sex = case_when(sex == 0 ~ "female",
                    sex == 1 ~ "male"),
    chest_pain_type = case_when(
      chest_pain_type == 1 ~ "typical angina",
      chest_pain_type == 2 ~ "atypical angina",
      chest_pain_type == 3 ~ "non-angina pain",
      chest_pain_type == 4 ~ "asymptomatic angina"
    ),
    fasting_blood_sugar =
      case_when(
        fasting_blood_sugar == 0 ~ "fasting blood sugar <= 120 mg/dl",
        fasting_blood_sugar == 1 ~ "fasting blood sugar > 120 mg/dl"
      ),
    resting_ecg = case_when(
      resting_ecg == 0 ~ "normal",
      resting_ecg == 1 ~ "ST-T wave abnormality",
      resting_ecg == 2 ~ "left ventricle hyperthrophy"
    ),
    exercise_induced_angina = 
      case_when(
       exercise_induced_angina ==  0 ~ 'no',
       exercise_induced_angina == 1 ~ "yes"
      ),
    peak_exercise_st_segment =
      case_when(
        peak_exercise_st_segment == 1 ~ "Up-sloaping",
        peak_exercise_st_segment == 2 ~ "Flat",
        peak_exercise_st_segment == 3 ~ "Down-sloaping"
      ),
    thalassemia =
      case_when(
        thalassemia == 3 ~ "normal",
        thalassemia  == 6 ~ "fixed defect",
        thalassemia == 7 ~ "reversible defect"
      ),
    diagnosis_heart_disease = 
      case_when(
        diagnosis_heart_disease == 0 ~ "absense",
        diagnosis_heart_disease >0 ~"present"
      ),
    across(where(is.character),as.factor)
  ) %>% 
  relocate(diagnosis_heart_disease)

skimr::skim_without_charts(hrt_data)[1:8,c(2,4,7)] %>% 
  knitr::kable(caption = "Data Discription(Factors)",
               col.names = c("variable","complete rate","fators count"))

skimr::skim_without_charts(hrt_data)[9:14,c(2,4,11,8,13)] %>% 
  knitr::kable(caption = "Data Discription(Numeric)",
               col.names = c("variable","complete rate","p25","mean","p75"))
```
 


```{r data_preprocess}
hrt_data = hrt_data %>% select(-location)
# split into training set
train_index = createDataPartition(hrt_data$diagnosis_heart_disease,p=0.8,list = F)

Y_tr = hrt_data$diagnosis_heart_disease[train_index]

Y_ts = hrt_data$diagnosis_heart_disease[-train_index]

options(na.action = "na.pass")
X_tr = model.matrix(diagnosis_heart_disease ~., hrt_data,na.action = "na.pass")[train_index,-1]

X_ts = model.matrix(diagnosis_heart_disease ~., hrt_data,na.action = "na.pass")[-train_index,-1]

TRC = caret::trainControl(method = "repeatedcv",repeats=5,
                          number = 5,
                          summaryFunction = twoClassSummary,
                          classProbs = T)
```


```{r explore}
featurePlot(x=X_tr,y=Y_tr,
            scales = list(x = list(relation = "free"),
                          y = list(relation = "free")),
            plot = "density",
            pch = "|",
            auto.key = list(columns = 2.5),
            main = list(label = "Data Distribution"),
            par.strip.text=list(cex=0.7),
            layout = c(4,5))
```



```{r logistic,cache=T}
set.seed(123123)
cl = parallel::makePSOCKcluster(5)
doParallel::registerDoParallel(cl)

logistic_model =
  train(
    X_tr,
    Y_tr,
    method = "glmnet",
    tuneGrid = expand.grid(alpha = seq(0,1,length=6),
                           lambda = exp(seq(
                             6, to = -6, length = 50
                           ))),
    family = "binomial",
    preProcess = c("knnImpute", "center", "scale"),
    metric = "ROC",
    trControl = TRC
  )

stopCluster(cl)

p_logistics =
  ggplot(logistic_model,highlight = T) +
  scale_x_continuous(trans = "log")+
  labs(title = "Lasso Logistics")
```

```{r MARS,cache=T}
set.seed(123123)
cl = parallel::makePSOCKcluster(5)
doParallel::registerDoParallel(cl)

mars_model = 
  train(X_tr,
        Y_tr,
        method = "earth",
        tuneGrid = expand.grid(degree = 1:3,
                               nprune = 5:20),
        preProcess = c("knnImpute", "center", "scale"),
        trControl = TRC,
        metric = "ROC")

stopCluster(cl)

p_mars = ggplot(mars_model,highlight = T) +labs(title ="MARS")
```

```{r knn,cache=T}
set.seed(123123)
cl = parallel::makePSOCKcluster(5)
doParallel::registerDoParallel(cl)

knn_model = 
  train(X_tr,
        Y_tr,
        method = "knn",
        tuneGrid = expand.grid(k = seq(10,60,2)),
        preProcess = c("knnImpute", "center", "scale"),
        trControl = TRC,
        metric = "ROC")

stopCluster(cl)

p_knn = ggplot(knn_model,highlight = T)+labs(title = "KNN")
```

```{r lda,cache=T}
set.seed(123123)
cl = parallel::makePSOCKcluster(5)
doParallel::registerDoParallel(cl)

lda_model =  train(
  X_tr,
  Y_tr,
  method = "lda",
  preProcess = c("knnImpute", "center", "scale"),
  trControl = TRC,
  metric = "ROC"
)

stopCluster(cl)
```

```{r qda,cache=T}
set.seed(123123)
cl = parallel::makePSOCKcluster(5)
doParallel::registerDoParallel(cl)

qda_model =  train(
  X_tr,
  Y_tr,
  method = "qda",
  preProcess = c("knnImpute", "center", "scale"),
  trControl = TRC,
  metric = "ROC"
)

stopCluster(cl)
```

```{r tree,cache=T}
set.seed(123123)
cl = parallel::makePSOCKcluster(5)
doParallel::registerDoParallel(cl)

tree_model =
  train(
    X_tr,
    Y_tr,
    method = "rpart",
    tuneGrid = expand.grid(
      cp = exp(seq(-7,-4,length=50))
    ),
    preProcess = c("center", "scale"),
    trControl = caret::trainControl(method = "repeatedcv",repeats=5,
                          number = 5,
                          summaryFunction = twoClassSummary,
                          classProbs = T,
                          selectionFunction = "oneSE"),
    metric = "ROC"
  )
stopCluster(cl)

p_tree = 
  ggplot(tree_model,highlight = T) + labs(title = "TREE")
```



```{r boosting,cache=T}
set.seed(123123)
cl = parallel::makePSOCKcluster(5)
doParallel::registerDoParallel(cl)
boosting = 
  train(X_tr,
        Y_tr,
        method = "gbm",
        tuneGrid = expand.grid(n.trees = c(10,50,100,1000,5000),
                               interaction.depth = 1:4,
                               shrinkage = c(0.01,0.05,0.5),
                               n.minobsinnode = c(1,5,10,20)),
        preProcess = c("knnImpute", "center", "scale"),
        trControl = TRC,
        verbose = F
        )

stopCluster(cl)
p_boost = ggplot(boosting,highlight =T) + labs(title = "BOOST TREE")
```


```{r random_forest,cache=T}
set.seed(123123)
cl = parallel::makePSOCKcluster(5)
doParallel::registerDoParallel(cl)
rforest = 
  train(X_tr,
        Y_tr,
        method = "ranger",
        tuneGrid = expand.grid(mtry =1:17,
                               splitrule = c("gini","extratrees"),
                               min.node.size = 1:5),
        preProcess = c("knnImpute", "center", "scale"),
        trControl = TRC,
        metric = "ROC"
        )

stopCluster(cl)
p_rforest = ggplot(rforest,highlight =T)+labs(title = "Random Forest")
```


```{r svm_r,cache=T}
set.seed(123123)
cl = parallel::makePSOCKcluster(5)
doParallel::registerDoParallel(cl)
svm_r = 
  train(X_tr,
        Y_tr,
        method = "svmRadial",
        tuneGrid = expand.grid(sigma = exp(seq(-10,0,len = 10)),
                               C = exp(seq(-10,3,len = 30))),
        preProcess = c("knnImpute", "center", "scale"),
        trControl = TRC,
        metric = "ROC"
        )

stopCluster(cl)
p_svm_r = ggplot(svm_r,highlight =T)
```


```{r svm_l,cache=T}
set.seed(123123)
cl = parallel::makePSOCKcluster(5)
doParallel::registerDoParallel(cl)
svm_l = 
  train(X_tr,
        Y_tr,
        method = "svmLinear",
        tuneGrid = expand.grid(C = exp(seq(-10,0,len = 20))),
        preProcess = c("knnImpute", "center", "scale"),
        trControl = TRC,
        metric = "ROC"
        )

stopCluster(cl)
p_svm_l = ggplot(svm_l,highlight =T)
```


```{r res}

p_mv = vip::vip(mars_model$finalModel) + labs(title = "MARS: Importance of predictor")

p_ll = ggplotify::as.ggplot(~plot_glmnet(logistic_model$finalModel))
p_ll = p_ll + labs(title = "Lasso Logistics Model") +
  xlim(c(0.1,1))+
  ylim(c(0.1,1))

(p_logistics / p_tree) | (p_mars/p_knn) + plot_layout(guides = "collect")

(p_boost/p_rforest) |(p_svm_l/p_svm_r) + plot_layout(guides = "collect")

```



```{r rsmp}
model_list = 
  list(
    logistic = logistic_model,
    MARS = mars_model,
    knn = knn_model,
    lda = lda_model,
    qda = qda_model,
    TREE = tree_model,
    BOOST = boosting,
    Random_Forest = rforest,
    SVM_Linear = svm_l,
    SVM_Radial = svm_r
    
  )

rsmp = resamples(
  model_list,
  metric = c("ROC", "Kappa")
)

#summary(rsmp)

bwplot(rsmp,metric = c("ROC","Sens"))
```

  In our trained model have similar ROC peformance excepted for `TREE` and `qda`. Considering our model is used for improving screening process, we would prefer model with higher sensitivity. Considering both metrics, MARS method which has high ROC and highest mean sensitivity is chosen as our model.

```{r explain_mars}
mars = DALEX::explain(mars_model,label = "MARS",data = X_tr,y = Y_tr %>% as.numeric(),verbose = F)
mars_important =  model_parts(mars)

mars_int = plot(mars_important)

plot(mars_int)
```
  
  

```{r ROC}
ROC =
  expand.grid(
    test_X = list(X_ts),
    test_Y = list(Y_ts),
    model = model_list
  ) %>%
  mutate(
    pred = map2(model, test_X,  ~ predict(.x, newdata = .y, type = "prob")[, 2]),
    roc = map2(test_Y, pred,  ~ pROC::roc(.x, .y))
  ) %>% 
  pull(roc)

auc = c()

for (i in 1:length(model_list)){
  auc = append(auc,ROC[[i]]$auc[1])
  plot(ROC[[i]],col = i, add = T * (i>1), legacy.axes = T * (i==1))
}

model_name = 
  c("lasso logistic","MARS","KNN","LDA","QDA","TREE","Boost","Random_Forest","SVM Linear","SVM Radial")

legend("bottomright",
       legend = paste0(model_name,"~",round(auc,3)),col=1:length(model_list),lwd=2)
```


